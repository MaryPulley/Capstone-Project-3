{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n",
      "Download complete!\n",
      "Extracting dataset...\n",
      "Extraction complete!\n",
      "Loaded 15515 images into DataLoader!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "import json\n",
    "from pathlib import Path\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "# Load Croissant metadata\n",
    "croissant_path = \"garbage-classification-metadata (1).json\"\n",
    "with open(croissant_path, \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Extract dataset URL\n",
    "dataset_url = metadata[\"distribution\"][0][\"contentUrl\"]\n",
    "dataset_zip = \"garbage_classification.zip\"\n",
    "extract_path = \"garbage_classification\"\n",
    "\n",
    "# Download dataset if not exists\n",
    "if not os.path.exists(dataset_zip):\n",
    "    print(\"Downloading dataset...\")\n",
    "    response = requests.get(dataset_url, stream=True)\n",
    "    with open(dataset_zip, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            f.write(chunk)\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "# Extract dataset if not already extracted\n",
    "if not os.path.exists(extract_path):\n",
    "    print(\"Extracting dataset...\")\n",
    "    with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    print(\"Extraction complete!\")\n",
    "\n",
    "# Define dataset transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load dataset with PyTorch DataLoader\n",
    "train_dataset = datasets.ImageFolder(root=extract_path, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f\"Loaded {len(train_dataset)} images into DataLoader!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15515 images into DataLoader!\n",
      "CNN Model initialized!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "# Load Croissant metadata\n",
    "croissant_path = \"garbage-classification-metadata (1).json\"\n",
    "with open(croissant_path, \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Extract dataset URL\n",
    "dataset_url = metadata[\"distribution\"][0][\"contentUrl\"]\n",
    "dataset_zip = \"garbage_classification.zip\"\n",
    "extract_path = \"garbage_classification\"\n",
    "\n",
    "# Download dataset if not exists\n",
    "if not os.path.exists(dataset_zip):\n",
    "    print(\"Downloading dataset...\")\n",
    "    response = requests.get(dataset_url, stream=True)\n",
    "    with open(dataset_zip, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            f.write(chunk)\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "# Extract dataset if not already extracted\n",
    "if not os.path.exists(extract_path):\n",
    "    print(\"Extracting dataset...\")\n",
    "    with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    print(\"Extraction complete!\")\n",
    "\n",
    "# Define dataset transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load dataset with PyTorch DataLoader\n",
    "train_dataset = datasets.ImageFolder(root=extract_path, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f\"Loaded {len(train_dataset)} images into DataLoader!\")\n",
    "\n",
    "# Define CNN Model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes=12):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 56 * 56, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = CNNModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"CNN Model initialized!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15515 images into DataLoader!\n",
      "CNN Model initialized!\n",
      "Epoch 1/10, Loss: 0.0052\n",
      "Epoch 2/10, Loss: 0.0000\n",
      "Epoch 3/10, Loss: 0.0000\n",
      "Epoch 4/10, Loss: 0.0000\n",
      "Epoch 5/10, Loss: 0.0000\n",
      "Epoch 6/10, Loss: 0.0000\n",
      "Epoch 7/10, Loss: 0.0000\n",
      "Epoch 8/10, Loss: 0.0000\n",
      "Epoch 9/10, Loss: 0.0000\n",
      "Epoch 10/10, Loss: 0.0000\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "# Load Croissant metadata\n",
    "croissant_path = \"garbage-classification-metadata (1).json\"\n",
    "with open(croissant_path, \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Extract dataset URL\n",
    "dataset_url = metadata[\"distribution\"][0][\"contentUrl\"]\n",
    "dataset_zip = \"garbage_classification.zip\"\n",
    "extract_path = \"garbage_classification\"\n",
    "\n",
    "# Download dataset if not exists\n",
    "if not os.path.exists(dataset_zip):\n",
    "    print(\"Downloading dataset...\")\n",
    "    response = requests.get(dataset_url, stream=True)\n",
    "    with open(dataset_zip, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            f.write(chunk)\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "# Extract dataset if not already extracted\n",
    "if not os.path.exists(extract_path):\n",
    "    print(\"Extracting dataset...\")\n",
    "    with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    print(\"Extraction complete!\")\n",
    "\n",
    "# Define dataset transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load dataset with PyTorch DataLoader\n",
    "train_dataset = datasets.ImageFolder(root=extract_path, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f\"Loaded {len(train_dataset)} images into DataLoader!\")\n",
    "\n",
    "# Define CNN Model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes=12):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 56 * 56, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = CNNModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"CNN Model initialized!\")\n",
    "\n",
    "# Training Loop\n",
    "def train(model, train_loader, criterion, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train(model, train_loader, criterion, optimizer, epochs=10)\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# What is the accuracy of the model on the training set?\n",
    "def evaluate(model, train_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in train_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy \n",
    "\n",
    "print(f\"Training Accuracy: {evaluate(model, train_loader):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
